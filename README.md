The python function is a rather simple approach to the problem. First the list of homes is sorted based on proximity to the subject home based on the lat and lon (pip install geopy). The sorted list is then truncated to n where n is the the number of similar listings. I believe proximity is the most important factor from a user perspective, since home seeker are most likely searching for listing in a similar area. The truncated list is then sorted based on a simple function factoring the bed, bath, stories, pool, dwelling_type differences. 

Part 2: 
To persist the data, I believe storing a reasonable number (perhaps some multiple of n with some upper limit) of listings sorted by proximity in a separate database / cache would be a solution. As new data comes in, its distance to the subject home can be compared to the last home on the sorted listings. If it is larger it should do nothing. If it is smaller, insert it into the list and discard the last home on the list. I believe that user would most likely be searching for houses in a similar area, so as long as this sorted list cover this similar area there should not be much of a need to update this list. As the user changes the subject house, there can be a check to see if the coverage is still relevant. If too many houses on the list are irrelevant, then a new list can be produced. However, this probably won’t happen, since most of the time the user won’t be changing subject houses of great distance. If the number of data points scales, I think the above method will still work. However, the overall data point may need to sorted into different regions. Thus if the coverage is the the entire United States, then the data might need to be separated into states. If the coverage is the world, then the data should be separated by country as well.  
